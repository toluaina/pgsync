# Environment variable override

# PGSync
# path to the application schema config
# SCHEMA='/path/to/schema.json'
# number of records to fetch from db at a time
# QUERY_CHUNK_SIZE=10000
# poll db interval (consider reducing this duration to increase throughput)
# POLL_TIMEOUT=0.1
# replication slot cleanup interval (in secs)
# REPLICATION_SLOT_CLEANUP_INTERVAL=180

# Elasticsearch
# ELASTICSEARCH_SCHEME=http
# ELASTICSEARCH_HOST=localhost
# ELASTICSEARCH_PORT=9200
# ELASTICSEARCH_USER=nobody
# ELASTICSEARCH_PASSWORD=PLEASE_REPLACE_ME
# increase this if you are getting read request timeouts
# ELASTICSEARCH_TIMEOUT=10
# number of documents to index at a time
# ELASTICSEARCH_CHUNK_SIZE=2000
# the maximum size of the request in bytes (default: 100MB)
# ELASTICSEARCH_MAX_CHUNK_BYTES=104857600
# the size of the threadpool to use for the bulk requests
# ELASTICSEARCH_THREAD_COUNT=4
# the size of the task queue between the main thread
# (producing chunks to send) and the processing threads.
# ELASTICSEARCH_QUEUE_SIZE=4

# Postgres
# PG_HOST=localhost
# PG_USER=i-am-root
# PG_PORT=5432
# PG_PASSWORD=PLEASE_REPLACE_ME
# PG_SSLMODE=require
# PG_SSLROOTCERT=/path/to/ca.cert

# Redis
# REDIS_HOST=localhost
# REDIS_PORT=6379
# REDIS_DB=0
# REDIS_AUTH=PLEASE_REPLACE_ME
# number of items to read from Redis at a time
# REDIS_CHUNK_SIZE=1000
# redis socket connection timeout
# REDIS_SOCKET_TIMEOUT=5
# REDIS_POLL_INTERVAL=0.01

# Logging
# CRITICAL - 50
# ERROR    - 40
# WARNING  - 30
# INFO     - 20
# DEBUG    - 10
CONSOLE_LOGGING_HANDLER_MIN_LEVEL=DEBUG
CUSTOM_LOGGING=elasticsearch=WARNING,pgsync=INFO

# New Relic
# NEW_RELIC_ENVIRONMENT=development
# NEW_RELIC_APP_NAME=PGSync
# NEW_RELIC_LOG_LEVEL=critical
# NEW_RELIC_LICENSE_KEY=*********
