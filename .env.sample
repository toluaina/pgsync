# Environment variable override

# PGSync
# path to the application schema config
# SCHEMA='/path/to/schema.json'
# URL for schema config
# SCHEMA_URL='http://xyz.com:8000/schema.json'
# url for s3 schema config
# S3_SCHEMA_URL='s3://my-bucket/path/to/schema.json'
# number of records to fetch from db at a time
# QUERY_CHUNK_SIZE=10000
# poll db interval (consider reducing this duration to increase throughput)
# POLL_TIMEOUT=0.1
# replication slot cleanup interval (in secs)
# REPLICATION_SLOT_CLEANUP_INTERVAL=180
# checkpoint file path
# CHECKPOINT_PATH=./
# block size for parallel sync
# BLOCK_SIZE=2048*10
# QUERY_LITERAL_BINDS=False
# number of threads to spawn for poll db
# NTHREADS_POLLDB=1
# batch size for LOGICAL_SLOT_CHANGES for minimizing tmp file disk usage
# LOGICAL_SLOT_CHUNK_SIZE=5000
# USE_ASYNC=False
# JOIN_QUERIES=False
# STREAM_RESULTS=True
# db polling interval
# POLL_INTERVAL=0.1
# FILTER_CHUNK_SIZE=5000
# store checkpoint in redis/valkey instead of on filesystem
# REDIS_CHECKPOINT=False
# FORMAT_WITH_COMMAS=True
# PG_DRIVER=psycopg2 # or pymysql
# USE_UTF8MB4=False

# SQLAlchemy Settings:
# This is the number of connections that will be persistently maintained in the pool. 
# SQLALCHEMY_POOL_SIZE = env.int("SQLALCHEMY_POOL_SIZE", default=5)
# This is the number of connections that can be opened beyond the pool_size when all connections in the pool are in use.
# SQLALCHEMY_MAX_OVERFLOW = env.int("SQLALCHEMY_MAX_OVERFLOW", default=2)
# When set to True, a "ping" will be performed on connections before they are checked out of the pool to ensure they are still live.
# SQLALCHEMY_POOL_PRE_PING = env.bool("SQLALCHEMY_POOL_PRE_PING", default=False)
# This means connections are not recycled based on a timeout. If set to a positive integer, connections will be recycled after that many seconds. For example, 3600 recycles connections after one hour.
# SQLALCHEMY_POOL_RECYCLE = env.int("SQLALCHEMY_POOL_RECYCLE", default=-1)
# # This is the number of seconds to wait for a connection to become available from the pool before raising a TimeoutError.
# SQLALCHEMY_POOL_TIMEOUT = env.int("SQLALCHEMY_POOL_TIMEOUT", default=30)

# Elasticsearch/Opensearch
# ELASTICSEARCH_SCHEME=http
# ELASTICSEARCH_HOST=localhost
# ELASTICSEARCH_PORT=9200
# ELASTICSEARCH_USER=nobody
# ELASTICSEARCH_PASSWORD=PLEASE_REPLACE_ME
# increase this if you are getting read request timeouts
# ELASTICSEARCH_TIMEOUT=10
# number of documents to index at a time
# ELASTICSEARCH_CHUNK_SIZE=5000
# the maximum size of the request in bytes (default: 100MB)
# ELASTICSEARCH_MAX_CHUNK_BYTES=104857600
# the size of the threadpool to use for the bulk requests
# ELASTICSEARCH_THREAD_COUNT=4
# the size of the task queue between the main thread
# (producing chunks to send) and the processing threads.
# ELASTICSEARCH_QUEUE_SIZE=4
# turn on SSL
# ELASTICSEARCH_USE_SSL=False
# don't show warnings about ssl certs verification
# ELASTICSEARCH_SSL_SHOW_WARN=False
# provide a path to CA certs on disk
# ELASTICSEARCH_CA_CERTS=/path/to/ca.cert
# PEM formatted SSL client certificate
# ELASTICSEARCH_CLIENT_CERT=/path/to/cert.pem
# PEM formatted SSL client key
# ELASTICSEARCH_CLIENT_KEY=/path/to/ssl.key
# ELASTICSEARCH_AWS_REGION=eu-west-1
# ELASTICSEARCH_AWS_HOSTED=True
# ELASTICSEARCH_STREAMING_BULK=False
# maximum number of times a document will be retried when ``429`` is received,
# set to 0 (default) for no retries on ``429``
# ELASTICSEARCH_MAX_RETRIES=0
# number of seconds we should wait before the first retry.
# Any subsequent retries will be powers of ``initial_backoff * 2**retry_number``
# ELASTICSEARCH_INITIAL_BACKOFF=2
# maximum number of seconds a retry will wait
# ELASTICSEARCH_MAX_BACKOFF=600
# if ``False`` then don't propagate exceptions from call to elasticsearch ``bulk``
# ELASTICSEARCH_RAISE_ON_EXCEPTION=True
# ELASTICSEARCH_RAISE_ON_ERROR=True
# ELASTICSEARCH_API_KEY_ID=PLEASE_REPLACE_ME
# ELASTICSEARCH_API_KEY=PLEASE_REPLACE_ME
# ELASTICSEARCH=true
# OPENSEARCH=false
# ELASTICSEARCH_URL takes precedence over the above variables
# ELASTICSEARCH_URL=http://localhost:9200
# ELASTICSEARCH_POOL_MAXSIZE=None

# Postgres
# PG_HOST=localhost
# PG_USER=i-am-root
# PG_PORT=5432
# PG_PASSWORD=PLEASE_REPLACE_ME
# PG_SSLMODE=require
# PG_SSLROOTCERT=/path/to/ca.cert
# PG_URL takes precedence over the above variables
# PG_URL=postgresql://i-am-root:PLEASE_REPLACE_ME@localhost:5432/i-am-root
# setting any of these enables read-only mode for consumer
# PG_HOST_RO=
# PG_USER_RO=
# PG_PASSWORD_RO=
# PG_PORT_RO=
# PG_URL_RO=
# the default database name e.g postges or defaultdb
# PG_DATABASE=postgres

# Redis/Valkey
# REDIS_HOST=localhost
# REDIS_PORT=6379
# REDIS_DB=0
# REDIS_AUTH=PLEASE_REPLACE_ME
# REDIS_USER=megatron
# number of items to read from Redis/Valkey at a time
# REDIS_READ_CHUNK_SIZE=1000
# number of items to write from Redis/Valkey at a time
# REDIS_WRITE_CHUNK_SIZE=1000
# redis/valkey socket connection timeout
# REDIS_SOCKET_TIMEOUT=5
# REDIS_POLL_INTERVAL=0.01
# REDIS_SCHEME=redis
# REDIS_URL takes precedence over the above variables
# REDIS_URL=redis://megatron:PLEASE_REPLACE_ME@localhost:6379/0
# REDIS_RETRY_ON_TIMEOUT=False

# Logging
# CRITICAL - 50
# ERROR    - 40
# WARNING  - 30
# INFO     - 20
# DEBUG    - 10
CONSOLE_LOGGING_HANDLER_MIN_LEVEL=DEBUG
CUSTOM_LOGGING=elasticsearch=WARNING,pgsync=INFO
LOG_INTERVAL=0.5

# New Relic
# NEW_RELIC_ENVIRONMENT=development
# NEW_RELIC_APP_NAME=PGSync
# NEW_RELIC_LOG_LEVEL=critical
# NEW_RELIC_LICENSE_KEY=*********
